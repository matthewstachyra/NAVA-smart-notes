{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e266492-869a-4ec6-99bf-6f55411fe2c3",
   "metadata": {},
   "source": [
    "### Tag generation prototype v0.2\n",
    "Created: 12 July 2022\n",
    "v0.2 created: 15 July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1fe03-3117-4fe1-b7fe-e9b9140ddcd4",
   "metadata": {},
   "source": [
    "parrot: https://github.com/PrithivirajDamodaran/Parrot_Paraphraser <br>\n",
    "rasa prototype: https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi#scrollTo=776uG9Q6DTnf <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b248868-8d9c-4b20-9804-a9306b942b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# nltk, spacy, gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# pytorch, parrot\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "\n",
    "# sentence transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0616cc9-de8c-44ab-8128-8cb079a1aa3e",
   "metadata": {},
   "source": [
    "## what will make up tags?\n",
    "#### current philosophy is to create as many unique but semantically related tokens as possible, and use those as tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529973d3-fe13-4fca-b051-c5b70955fbf5",
   "metadata": {},
   "source": [
    "#### WIP tags generated\n",
    "1. tokens from words in note\n",
    "2. tokens from paraphrases of note\n",
    "3. synonyms of nouns in note\n",
    "4. synonyms of nouns in each paraphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d5976-1712-4946-880f-62eda888bd46",
   "metadata": {},
   "source": [
    "## using `parrot` paraphraser to generate paraphrases, demo below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10375bc2-9d1c-4868-b9ed-2784adecf3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf493fd-f5e9-447e-9cc6-59696f662d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  Can you recommed some upscale restaurants in Newyork?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('list some upscale restaurants in newyork?', 27)\n",
      "('can you recommend some of the best restaurants in newyork?', 23)\n",
      "('can you recommend some highend restaurants in newyork?', 20)\n",
      "('can you recommend some good upscale restaurants in new york?', 19)\n",
      "('can you recommend some upscale restaurants in new york?', 14)\n",
      "('can you recommend some upscale restaurants in newyork?', 13)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  I was sad that I failed my exam but I knew that I could score better next time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('i was sad to miss the exam but i knew i could score a little better next time', 40)\n",
      "('i was sad that i had failed my exam but i knew i could do better next time', 26)\n",
      "('i was sad to fail my exam but i knew i could score better next time', 25)\n",
      "('i was sad that i failed my exam but i knew i could score better next time', 18)\n",
      "('i was sad that i failed my exam but i knew that i could score better next time', 13)\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"Can you recommed some upscale restaurants in Newyork?\",\n",
    "           \"I was sad that I failed my exam but I knew that I could score better next time.\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    print(\"-\"*100)\n",
    "    print(\"Input_phrase: \", phrase)\n",
    "    print(\"-\"*100)\n",
    "    para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False)\n",
    "    for para_phrase in para_phrases:\n",
    "        print(para_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18457761-df7b-4769-9512-1a14225c5610",
   "metadata": {},
   "source": [
    "## creating classes to establish pipeline: preprocessing text, generating synonyms, generating paraphrases, generating tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0ef484-aa8a-4aec-8316-7372d8d8852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (63.1.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/matthewstachyra/.local/share/virtualenvs/NAVA-dJ29zUSD/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022c6afe-fd57-48ce-a64b-ce90097306b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, note, remove_stopwords=False, lemmatize=False):\n",
    "        if not note: raise ValueError(\"Error: Input is invalid. It should be a string.\")\n",
    "        self.note = note\n",
    "        self.stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def __call__(self):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        re_strip = re.compile('<.*?>')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.note = self.note.lower()\n",
    "        self.note = re.sub(re_strip, '', self.note)\n",
    "        self.note = re.sub('[0-9]+', '', self.note)\n",
    "        self.note = \" \".join(tokenizer.tokenize(self.note))\n",
    "        \n",
    "        if self.lemmatize:\n",
    "            self.note = \"\".join([lemmatizer.lemmatize(word) for word in self.note])\n",
    "        \n",
    "        if self.stopwords:\n",
    "            return self.remove_stopwords()\n",
    "        else:\n",
    "            return self.note\n",
    "    \n",
    "    def remove_stopwords(self):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        return \" \".join([word for word in self.note.split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc78234f-674b-436e-b425-c9d7afefeae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test note'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Preprocessor(\"This is a test note!@$#!@#!\")\n",
    "p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ecd3168-edf9-4469-a19c-4499f0db883b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test note'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Preprocessor(\"This is a test note!@$#!@#!\", remove_stopwords=True)\n",
    "p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28fe311-b733-489e-b358-7c09cb97d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synonymizer:\n",
    "    def __init__(self, note):\n",
    "        self.preprocessor = Preprocessor(note)\n",
    "        self.note = self.preprocessor()\n",
    "        self.glovemodel = api.load(\"glove-wiki-gigaword-100\")\n",
    "        self.spacymodel = spacy.load(\"en_core_web_sm\")\n",
    "        self.posmap = {'VERB':'v', 'NOUN':'n', 'PRON':'n', 'PROPN':'n', 'ADJ':'a', 'ADV':'r'}\n",
    "        \n",
    "    def __call__(self):\n",
    "        '''return dictionary of words : synonym(s) pairs.\n",
    "        '''\n",
    "        # NOTE current version removes n grams.\n",
    "        d = {}\n",
    "\n",
    "        for word in self.note.split():\n",
    "            synonyms = self.synonyms_by_word(word)\n",
    "\n",
    "            if synonyms: synonyms = list(set([synonym\n",
    "                                     for synonym in synonyms\n",
    "                                     if len(synonym.split(\"_\"))==1]))\n",
    "\n",
    "            if synonyms: d[word] = synonyms\n",
    "            \n",
    "        return d\n",
    "        \n",
    "    def pos_by_word(self, word):\n",
    "        for w in self.spacymodel(self.note):\n",
    "            if str(w)==word: return w.pos_\n",
    "\n",
    "    def similarities_by_word(self, word, synonyms):\n",
    "        def cosinesim(v1, v2):\n",
    "            return (np.dot(v1, v2 / (norm(v1) * norm(v2))))\n",
    "        \n",
    "        def embed(vector, model):\n",
    "            try:\n",
    "                vec = self.glovemodel.get_vector(vector)\n",
    "            except:\n",
    "                return np.empty(0)\n",
    "            return vec\n",
    "\n",
    "        sims = {word: 1.0}\n",
    "        ref  = embed(word, self.glovemodel)\n",
    "\n",
    "        for s in synonyms:\n",
    "            vec = embed(s, self.glovemodel)\n",
    "            if vec.any():\n",
    "                sim     = cosinesim(ref, vec)\n",
    "                sims[s] = sim\n",
    "\n",
    "        return sims\n",
    "\n",
    "    def print_similarities(self, similarities):\n",
    "        for synonym, similarity in similarities.items():\n",
    "            print(f\"word: {synonym}, cosine similarity: {similarity}\")\n",
    "\n",
    "    def synonyms_by_word(self, word):\n",
    "        pos = self.pos_by_word(word)\n",
    "\n",
    "        if pos not in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']: return\n",
    "\n",
    "        # get full set of synonyms\n",
    "        synonyms = set(list(itertools.chain([synonym\n",
    "                                             for synset in wn.synsets(word, pos=self.posmap[pos])\n",
    "                                             for synonym in synset.lemma_names()\n",
    "                                             if len(word)>1])))\n",
    "\n",
    "        # filter this set using cosine similarities\n",
    "        similarities = self.similarities_by_word(word, synonyms)\n",
    "\n",
    "        return [synonym\n",
    "                for synonym, similarity in similarities.items()\n",
    "                if similarity>=0.70]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92437d5b-c8e9-4646-baad-52d2b9c1de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Synonymizer(\"I was sad that I failed my exam but I knew that I could score better next time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ca4004-f350-4502-91ca-752fad4a155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': ['i'],\n",
       " 'sad': ['sorry', 'sad'],\n",
       " 'failed': ['fail', 'failed'],\n",
       " 'my': ['my'],\n",
       " 'exam': ['examination', 'exam'],\n",
       " 'knew': ['know', 'knew'],\n",
       " 'score': ['score'],\n",
       " 'better': ['best', 'good', 'better', 'well'],\n",
       " 'next': ['next', 'future'],\n",
       " 'time': ['time']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50edc611-764f-4fb9-8f67-68818e4b5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paraphraser:\n",
    "    def __init__(self, note, with_gpu=False):\n",
    "        self.paraphrases = []\n",
    "        self.preprocessor = Preprocessor(note)\n",
    "        self.synonymizer = Synonymizer(note)\n",
    "        self.parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\")\n",
    "        self.note = self.preprocessor()\n",
    "        self.synonyms = self.synonymizer() \n",
    "        self.gpu = with_gpu\n",
    "    \n",
    "    def __call__(self):\n",
    "        '''return list of paraphrases of the note.\n",
    "        '''\n",
    "        self.transformer_phrases()\n",
    "        self.synonym_phrases()\n",
    "        random.shuffle(self.paraphrases)\n",
    "        return self.paraphrases\n",
    "        \n",
    "    def transformer_phrases(self):\n",
    "        phrases = [tup[0] for tup in parrot.augment(input_phrase=self.note, \n",
    "                                                 use_gpu=self.gpu)]\n",
    "        self.paraphrases.extend(phrases)\n",
    "        \n",
    "    def synonym_phrases(self):\n",
    "        genlist = []\n",
    "        tokens  = []\n",
    "        prev    = 0\n",
    "\n",
    "        for word in self.note.split():\n",
    "            if word in self.synonyms:\n",
    "                tokens.append(list(itertools.chain(*[[word], self.synonyms[word]])))\n",
    "            else:\n",
    "                tokens.append([word])\n",
    "\n",
    "        # use tokens to return new utterances\n",
    "        for i in range(len(tokens)):\n",
    "            word  = tokens[i][0]\n",
    "            slist = tokens[i]\n",
    "\n",
    "            for j in range(len(slist)):\n",
    "                start = self.note.find(word, prev)\n",
    "                end   = start + len(word)\n",
    "                gen   = self.note[:start] + slist[j] + self.note[end:]\n",
    "                genlist.append(gen)\n",
    "\n",
    "            prev = end\n",
    "\n",
    "        self.paraphrases.extend(list(set(genlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb63cc1d-8a3f-474d-9d44-af4e96a98106",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Paraphraser(\"I was sad that I failed my exam but I knew that I could score better next time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3578bbba-5985-426a-8416-ed417dd8ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i was sad to fail an exam but i knew i could get better next time', 'i was sad that i failed my exams but i knew i would be able to score better next time', 'i was sad that i failed the exams but i knew i could score better next time', 'i was sad that i failed my exam but i knew i could score better the next time', 'i was sad that i failed my exams but i knew i could score better next time', 'i was sad that i failed my exam but i knew i could score better next time', 'i was sad i failed my exam but i knew that i could score better next time', 'i was sad that i failed my exam but i knew that i could score better next time']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i was sad to fail an exam but i knew i could get better next time',\n",
       " 'i was sad that i failed my exams but i knew i would be able to score better next time',\n",
       " 'i was sad that i failed the exams but i knew i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew i could score better the next time',\n",
       " 'i was sad that i failed my exams but i knew i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew i could score better next time',\n",
       " 'i was sad i failed my exam but i knew that i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew that i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew that i could score better future time',\n",
       " 'i was sad that i failed my exam but i knew that i could score better next time',\n",
       " 'i was sorry that i failed my exam but i knew that i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew that i could score well next time',\n",
       " 'i was sad that i failed my exam but i knew that i could score best next time',\n",
       " 'i was sad that i failed my exam but i know that i could score better next time',\n",
       " 'i was sad that i failed my exam but i knew that i could score good next time',\n",
       " 'i was sad that i fail my exam but i knew that i could score better next time',\n",
       " 'i was sad that i failed my examination but i knew that i could score better next time']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19de3e2-c7ad-4d7f-ba4f-c07d13791170",
   "metadata": {},
   "source": [
    "Tags\n",
    "- note\n",
    "- cleaned note split\n",
    "- paraphrases\n",
    "- cleaned paraphrases split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9155c32-c2b7-42ae-a195-f640f49a5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger:\n",
    "    def __init__(self, note, with_gpu=False):\n",
    "        self.tags = [note]\n",
    "        self.paraphraser = Paraphraser(note, with_gpu)\n",
    "        self.note = self.paraphraser.note\n",
    "        self.preprocessor = Preprocessor(note, lemmatize=True, remove_stopwords=True)\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.note_tags()\n",
    "        self.paraphrase_tags()\n",
    "        return list(set(self.tags))\n",
    "\n",
    "    def note_tags(self):\n",
    "        self.tags.append(self.preprocessor())\n",
    "        \n",
    "    def paraphrase_tags(self):\n",
    "        for p in self.paraphraser():\n",
    "            self.tags.append(p)\n",
    "            c = Preprocessor(p, lemmatize=True, remove_stopwords=True)\n",
    "            self.tags.extend(c().split())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af5416f7-e30c-4218-8fda-c2765f9fa7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tagger(\"My uncle took me to get ice cream. I learned that I love chocolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74ba1996-b5c5-42d8-91ba-c0205807561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my uncle have me to get ice cream i learned that i love chocolate',\n",
       " 'my uncle took me out for ice cream i discovered that i love chocolates',\n",
       " 'my uncle took me to get ice cream i learn that i love chocolate',\n",
       " 'start',\n",
       " 'uncle took get ice cream learned love chocolate',\n",
       " 'come',\n",
       " 'my uncle took me to let ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to go ice cream i learned that i love chocolate',\n",
       " 'let',\n",
       " 'bring',\n",
       " 'find',\n",
       " 'took',\n",
       " 'love',\n",
       " 'chocolates',\n",
       " 'my uncle took me to find ice cream i learned that i love chocolate',\n",
       " 'learn',\n",
       " 'my uncle took me to take ice cream i learned that i love chocolate',\n",
       " 'chocolate',\n",
       " 'my uncle took me to the ice cream shop and i learned that i love chocolate',\n",
       " 'shop',\n",
       " 'learned',\n",
       " 'get',\n",
       " 'uncle',\n",
       " 'my uncle took me to ice cream and i learned that i love chocolate',\n",
       " 'make',\n",
       " 'My uncle took me to get ice cream. I learned that I love chocolate',\n",
       " 'ice',\n",
       " 'my uncle take me to get ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to have ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to get ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to make ice cream i learned that i love chocolate',\n",
       " 'go',\n",
       " 'my uncle took me to start ice cream i learned that i love chocolate',\n",
       " 'my uncle took me for ice cream i learned that i love chocolate',\n",
       " 'take',\n",
       " 'cream',\n",
       " 'my uncle took me to bring ice cream i learned that i love chocolate',\n",
       " 'my uncle took me to come ice cream i learned that i love chocolate',\n",
       " 'discovered']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debcb8b-bf05-446d-be98-1c31b2ae01f8",
   "metadata": {},
   "source": [
    "### trial `sentence-transformers` for asymmetric semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ed3aff3-4161-4fab-b3c8-f0d407300380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011944055557250977,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1175,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ba14bf305f4aad905c782f93e9b5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008054971694946289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 190,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c07f3dc4b3d4ef388228fa756618abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007222890853881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 10610,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e545ea2301a34c4fa0b34515213bfc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008201122283935547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 612,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e389db2cd0ed4561b8823e1b6eddc39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08854103088378906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 116,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c6291851194334b3fc630ed27bd52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007688045501708984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 39265,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86d5fa92acc4ba196c0ac904102998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008329153060913086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 90888945,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8900ae3ede648f0b3770a11014dabb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007528066635131836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 53,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d7f6075d948ddbee0cbefd54b4f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007902383804321289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a07e0bcc8394c7e9ef59c614b1ca556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008401870727539062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 466247,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd0ccef0a7e4126ba39f5a7e6074a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007501125335693359,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 350,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f8e04250f44f40be5d7c4a6b89a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00809478759765625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 13156,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a28f9b113c14475998868727daddaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00704503059387207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a915461814d4687a6cd9d3182a5145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007070302963256836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 349,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb6d512b09438e98e77c0fe6dc8e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f13cb976-8c0e-41c6-ba2c-d1a86ce41b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.',\n",
    "          'apples, oranges, eggs, chicken'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75b1991a-28a9-443e-a664-ec43c410836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65b22358-b3b8-49a7-801a-d92685ea0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0332,  0.0044, -0.0063,  ...,  0.0692, -0.0246, -0.0376],\n",
       "        [ 0.0525,  0.0552, -0.0112,  ..., -0.0162, -0.0602, -0.0412],\n",
       "        [-0.0363, -0.0357, -0.0272,  ..., -0.0386,  0.1057, -0.0013],\n",
       "        ...,\n",
       "        [ 0.0235, -0.0585,  0.0560,  ...,  0.0583,  0.0377,  0.0410],\n",
       "        [ 0.0228,  0.1041, -0.0340,  ...,  0.0029,  0.0386,  0.0438],\n",
       "        [ 0.0042,  0.0007, -0.0168,  ...,  0.0888,  0.1127,  0.0361]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e66415a8-0be2-4275-8374-93c6f0e93b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'grocery list' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08eb7b1b-1999-49b3-9412-d4ac9b41ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedder.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8affccf8-6036-49d2-9abf-2062da2c1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5, score_function=util.dot_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40a47ddd-93e6-4c2c-ad36-d98a1b470916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 9, 'score': 0.3955247104167938},\n",
       "  {'corpus_id': 0, 'score': 0.21664908528327942},\n",
       "  {'corpus_id': 1, 'score': 0.19876854121685028},\n",
       "  {'corpus_id': 8, 'score': 0.06794281303882599},\n",
       "  {'corpus_id': 2, 'score': 0.050490930676460266}]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fb76294-5de1-44e7-a572-8e7cdbdff2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples, oranges, eggs, chicken\n",
      "A man is eating food.\n"
     ]
    }
   ],
   "source": [
    "# hit 1\n",
    "print(corpus[hits[0][0]['corpus_id']])\n",
    "\n",
    "# hit 2\n",
    "print(corpus[hits[0][1]['corpus_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaec311-5378-48c3-852a-2e0712bcf993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41faf9-1dbf-4deb-bbc4-ebd69df144be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
